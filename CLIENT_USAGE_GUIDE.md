# í´ë¼ì´ì–¸íŠ¸ì—ì„œ LLM ì‚¬ìš© ê°€ì´ë“œ

ì´ í”„ë¡œì íŠ¸ì˜ APIë¥¼ í†µí•´ í´ë¼ì´ì–¸íŠ¸ì—ì„œ LLMì„ ì‚¬ìš©í•˜ëŠ” ì™„ì „í•œ ê°€ì´ë“œì…ë‹ˆë‹¤.

## í˜„ì¬ í”„ë¡œì íŠ¸ì˜ API ì„œë²„

ì´ í”„ë¡œì íŠ¸ì—ëŠ” **2ê°œì˜ API ì„œë²„**ê°€ ìˆìŠµë‹ˆë‹¤:

### 1. Inference API (ì¶”ë¡ /ì‚¬ìš©) - `api_server.py`
- **ìš©ë„**: í•™ìŠµëœ ëª¨ë¸ë¡œ ì¶”ë¡ /ì§ˆë¬¸ ì‘ë‹µ
- **í¬íŠ¸**: 8000 (ê¸°ë³¸)
- **ê¸°ëŠ¥**: í…ìŠ¤íŠ¸ ìƒì„±, ì±„íŒ…

### 2. Training API (í•™ìŠµ ê´€ë¦¬) - `training_api.py`  
- **ìš©ë„**: í•™ìŠµ ì‘ì—… ì‹œì‘/ê´€ë¦¬
- **í¬íŠ¸**: 8001 (ê¸°ë³¸)
- **ê¸°ëŠ¥**: SFT/DPO í•™ìŠµ, ì‘ì—… ëª¨ë‹ˆí„°ë§

---

## ë¹ ë¥¸ ì‹œì‘ (3ë¶„)

### 1ë‹¨ê³„: API ì„œë²„ ì‹œì‘

```bash
# í„°ë¯¸ë„ 1: Inference API ì‹œì‘
python src/api_server.py \
    --model_path "outputs/checkpoints/final_model" \
    --port 8000
```

ì„œë²„ ì‹œì‘ í™•ì¸:
```
INFO: Uvicorn running on http://0.0.0.0:8000
```

### 2ë‹¨ê³„: í´ë¼ì´ì–¸íŠ¸ì—ì„œ ì‚¬ìš©

#### Python í´ë¼ì´ì–¸íŠ¸

```python
import requests

# API ì„œë²„ URL
API_URL = "http://localhost:8000"

# 1. í—¬ìŠ¤ì²´í¬
response = requests.get(f"{API_URL}/health")
print(response.json())
# {'status': 'healthy', 'model_loaded': True, ...}

# 2. ì§ˆë¬¸í•˜ê¸°
response = requests.post(
    f"{API_URL}/chat",
    json={
        "instruction": "Pythonì´ë€ ë¬´ì—‡ì¸ê°€ìš”?",
        "max_new_tokens": 200,
        "temperature": 0.7
    }
)

result = response.json()
print(result['response'])
```

#### cURLë¡œ í…ŒìŠ¤íŠ¸

```bash
# í—¬ìŠ¤ì²´í¬
curl http://localhost:8000/health

# ì§ˆë¬¸í•˜ê¸°
curl -X POST http://localhost:8000/chat \
  -H "Content-Type: application/json" \
  -d '{
    "instruction": "Pythonì˜ ì¥ì ì€?",
    "max_new_tokens": 200
  }'
```

---

## í´ë¼ì´ì–¸íŠ¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©

### ì œê³µë˜ëŠ” Python í´ë¼ì´ì–¸íŠ¸

ì´ í”„ë¡œì íŠ¸ì—ëŠ” `scripts/api_client.py`ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤!

```python
from scripts.api_client import LLMClient

# í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
client = LLMClient("http://localhost:8000")

# í—¬ìŠ¤ì²´í¬
health = client.health_check()
print(f"ì„œë²„ ìƒíƒœ: {health['status']}")

# ì§ˆë¬¸í•˜ê¸°
result = client.chat(
    instruction="ë¨¸ì‹ ëŸ¬ë‹ì´ë€?",
    max_new_tokens=200,
    temperature=0.7
)

print(result['response'])
```

---

## ì‹¤ì „ ì˜ˆì œ

### ì˜ˆì œ 1: ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ (Flask)

```python
# app.py
from flask import Flask, request, jsonify
import requests

app = Flask(__name__)
LLM_API_URL = "http://localhost:8000"

@app.route('/ask', methods=['POST'])
def ask_question():
    """ì‚¬ìš©ì ì§ˆë¬¸ ì²˜ë¦¬"""
    data = request.json
    question = data.get('question')
    
    # LLM API í˜¸ì¶œ
    response = requests.post(
        f"{LLM_API_URL}/chat",
        json={
            "instruction": question,
            "max_new_tokens": 200
        }
    )
    
    result = response.json()
    return jsonify({
        'question': question,
        'answer': result['response']
    })

if __name__ == '__main__':
    app.run(port=5000)
```

ì‚¬ìš©:
```bash
# ì„œë²„ ì‹œì‘
python app.py

# ìš”ì²­
curl -X POST http://localhost:5000/ask \
  -H "Content-Type: application/json" \
  -d '{"question": "Pythonì´ë€?"}'
```

### ì˜ˆì œ 2: React í”„ë¡ íŠ¸ì—”ë“œ

```javascript
// ChatComponent.jsx
import React, { useState } from 'react';

function ChatComponent() {
  const [question, setQuestion] = useState('');
  const [answer, setAnswer] = useState('');
  const [loading, setLoading] = useState(false);

  const askQuestion = async () => {
    setLoading(true);
    
    try {
      const response = await fetch('http://localhost:8000/chat', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          instruction: question,
          max_new_tokens: 200,
          temperature: 0.7
        })
      });
      
      const data = await response.json();
      setAnswer(data.response);
    } catch (error) {
      console.error('Error:', error);
    }
    
    setLoading(false);
  };

  return (
    <div>
      <input
        value={question}
        onChange={(e) => setQuestion(e.target.value)}
        placeholder="ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”"
      />
      <button onClick={askQuestion} disabled={loading}>
        {loading ? 'ìƒì„± ì¤‘...' : 'ì§ˆë¬¸í•˜ê¸°'}
      </button>
      {answer && <div><strong>ë‹µë³€:</strong> {answer}</div>}
    </div>
  );
}
```

### ì˜ˆì œ 3: Python ì±„íŒ…ë´‡

```python
# chatbot.py
from scripts.api_client import LLMClient

def chatbot():
    """ê°„ë‹¨í•œ ì±„íŒ…ë´‡"""
    client = LLMClient("http://localhost:8000")
    
    print("ì±„íŒ…ë´‡ì„ ì‹œì‘í•©ë‹ˆë‹¤! (ì¢…ë£Œ: 'quit')")
    
    while True:
        question = input("\në‹¹ì‹ : ").strip()
        
        if question.lower() in ['quit', 'exit', 'q']:
            print("ì±„íŒ…ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        
        if not question:
            continue
        
        try:
            result = client.chat(
                instruction=question,
                max_new_tokens=200
            )
            print(f"AI: {result['response']}")
        
        except Exception as e:
            print(f"ì˜¤ë¥˜: {e}")

if __name__ == "__main__":
    chatbot()
```

ì‹¤í–‰:
```bash
python chatbot.py
```

### ì˜ˆì œ 4: Streamlit ëŒ€ì‹œë³´ë“œ

```python
# dashboard.py
import streamlit as st
import requests

st.title("LLM ì±„íŒ… ëŒ€ì‹œë³´ë“œ")

# API URL
API_URL = "http://localhost:8000"

# ì§ˆë¬¸ ì…ë ¥
question = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:")

# íŒŒë¼ë¯¸í„° ì„¤ì •
col1, col2 = st.columns(2)
with col1:
    max_tokens = st.slider("Max Tokens", 50, 500, 200)
with col2:
    temperature = st.slider("Temperature", 0.0, 2.0, 0.7)

# ì§ˆë¬¸í•˜ê¸° ë²„íŠ¼
if st.button("ì§ˆë¬¸í•˜ê¸°"):
    if question:
        with st.spinner("ìƒì„± ì¤‘..."):
            response = requests.post(
                f"{API_URL}/chat",
                json={
                    "instruction": question,
                    "max_new_tokens": max_tokens,
                    "temperature": temperature
                }
            )
            
            result = response.json()
            st.success("ì™„ë£Œ!")
            st.write("**ë‹µë³€:**")
            st.write(result['response'])
            st.info(f"ìƒì„± ì‹œê°„: {result['generation_time']:.2f}ì´ˆ")
```

ì‹¤í–‰:
```bash
pip install streamlit
streamlit run dashboard.py
```

---

## ë‹¤ì–‘í•œ ì–¸ì–´ì—ì„œ ì‚¬ìš©

### JavaScript/TypeScript

```javascript
// api-client.js
class LLMClient {
  constructor(baseUrl = 'http://localhost:8000') {
    this.baseUrl = baseUrl;
  }

  async chat(instruction, options = {}) {
    const response = await fetch(`${this.baseUrl}/chat`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        instruction,
        max_new_tokens: options.maxTokens || 200,
        temperature: options.temperature || 0.7
      })
    });

    const data = await response.json();
    return data.response;
  }

  async health() {
    const response = await fetch(`${this.baseUrl}/health`);
    return await response.json();
  }
}

// ì‚¬ìš©
const client = new LLMClient();
const answer = await client.chat('Pythonì´ë€?');
console.log(answer);
```

### Java

```java
// LLMClient.java
import java.net.http.*;
import java.net.URI;
import org.json.*;

public class LLMClient {
    private String baseUrl;
    private HttpClient client;
    
    public LLMClient(String baseUrl) {
        this.baseUrl = baseUrl;
        this.client = HttpClient.newHttpClient();
    }
    
    public String chat(String instruction, int maxTokens) throws Exception {
        JSONObject request = new JSONObject();
        request.put("instruction", instruction);
        request.put("max_new_tokens", maxTokens);
        
        HttpRequest httpRequest = HttpRequest.newBuilder()
            .uri(URI.create(baseUrl + "/chat"))
            .header("Content-Type", "application/json")
            .POST(HttpRequest.BodyPublishers.ofString(request.toString()))
            .build();
        
        HttpResponse<String> response = client.send(
            httpRequest,
            HttpResponse.BodyHandlers.ofString()
        );
        
        JSONObject result = new JSONObject(response.body());
        return result.getString("response");
    }
}

// ì‚¬ìš©
LLMClient client = new LLMClient("http://localhost:8000");
String answer = client.chat("Pythonì´ë€?", 200);
System.out.println(answer);
```

### Go

```go
// llm_client.go
package main

import (
    "bytes"
    "encoding/json"
    "net/http"
)

type LLMClient struct {
    BaseURL string
}

type ChatRequest struct {
    Instruction  string  `json:"instruction"`
    MaxNewTokens int     `json:"max_new_tokens"`
    Temperature  float64 `json:"temperature"`
}

type ChatResponse struct {
    Response string `json:"response"`
}

func (c *LLMClient) Chat(instruction string) (string, error) {
    reqBody := ChatRequest{
        Instruction:  instruction,
        MaxNewTokens: 200,
        Temperature:  0.7,
    }
    
    jsonData, _ := json.Marshal(reqBody)
    
    resp, err := http.Post(
        c.BaseURL+"/chat",
        "application/json",
        bytes.NewBuffer(jsonData),
    )
    if err != nil {
        return "", err
    }
    defer resp.Body.Close()
    
    var result ChatResponse
    json.NewDecoder(resp.Body).Decode(&result)
    
    return result.Response, nil
}

// ì‚¬ìš©
client := &LLMClient{BaseURL: "http://localhost:8000"}
answer, _ := client.Chat("Pythonì´ë€?")
fmt.Println(answer)
```

---

## API ì—”ë“œí¬ì¸íŠ¸

### Inference API (í¬íŠ¸ 8000)

| ì—”ë“œí¬ì¸íŠ¸ | ë©”ì„œë“œ | ì„¤ëª… |
|-----------|--------|------|
| `/health` | GET | ì„œë²„ ìƒíƒœ í™•ì¸ |
| `/chat` | POST | ëŒ€í™”í˜• ì§ˆì˜ (Instruction í˜•ì‹) |
| `/generate` | POST | ì¼ë°˜ í…ìŠ¤íŠ¸ ìƒì„± |
| `/model_info` | GET | ëª¨ë¸ ì •ë³´ |
| `/load_model` | POST | ìƒˆ ëª¨ë¸ ë¡œë”© |

### Training API (í¬íŠ¸ 8001)

| ì—”ë“œí¬ì¸íŠ¸ | ë©”ì„œë“œ | ì„¤ëª… |
|-----------|--------|------|
| `/train/sft` | POST | SFT í•™ìŠµ ì‹œì‘ |
| `/train/dpo` | POST | DPO í•™ìŠµ ì‹œì‘ |
| `/jobs` | GET | í•™ìŠµ ì‘ì—… ëª©ë¡ |
| `/jobs/{id}` | GET | ì‘ì—… ìƒì„¸ ì •ë³´ |
| `/jobs/{id}/logs` | GET | í•™ìŠµ ë¡œê·¸ |

---

## í…ŒìŠ¤íŠ¸

### 1. API ì„œë²„ í…ŒìŠ¤íŠ¸

```bash
# ì œê³µëœ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
python scripts/test_api.py --mode all
```

### 2. ëŒ€í™”í˜• í…ŒìŠ¤íŠ¸

```bash
python scripts/test_api.py --mode interactive
```

### 3. í´ë¼ì´ì–¸íŠ¸ ì˜ˆì œ ì‹¤í–‰

```bash
python scripts/api_client.py
```

---

## í”„ë¡œë•ì…˜ ë°°í¬

### Docker ì»¨í…Œì´ë„ˆ

```dockerfile
# Dockerfile
FROM python:3.10-slim

WORKDIR /app
COPY requirements.txt requirements_api.txt ./
RUN pip install -r requirements.txt -r requirements_api.txt

COPY src/ ./src/
COPY models/ ./models/

EXPOSE 8000

CMD ["python", "src/api_server.py", \
     "--model_path", "models/your-model", \
     "--host", "0.0.0.0", \
     "--port", "8000"]
```

ë¹Œë“œ ë° ì‹¤í–‰:
```bash
docker build -t llm-api .
docker run -p 8000:8000 --gpus all llm-api
```

### Nginx ë¦¬ë²„ìŠ¤ í”„ë¡ì‹œ

```nginx
# /etc/nginx/sites-available/llm-api
server {
    listen 80;
    server_name api.yourdomain.com;

    location / {
        proxy_pass http://localhost:8000;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_read_timeout 300s;
    }
}
```

---

## ì™„ì „í•œ ì›Œí¬í”Œë¡œìš°

### ì‹œë‚˜ë¦¬ì˜¤: ì›¹ ì„œë¹„ìŠ¤ êµ¬ì¶•

```bash
# 1. ëª¨ë¸ í•™ìŠµ (í•œ ë²ˆë§Œ)
python src/train.py --config configs/train_config.yaml

# 2. API ì„œë²„ ì‹œì‘ (ë°±ê·¸ë¼ìš´ë“œ)
nohup python src/api_server.py \
    --model_path "outputs/checkpoints/final_model" \
    --port 8000 > api.log 2>&1 &

# 3. ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘
python your_webapp.py

# 4. í´ë¼ì´ì–¸íŠ¸ì—ì„œ ì‚¬ìš©
# ì›¹, ëª¨ë°”ì¼, CLI ë“± ì–´ë””ì„œë“  API í˜¸ì¶œ
```

---

## ìš”ì•½

### âœ… ë„¤, ì™„ì „íˆ ê°€ëŠ¥í•©ë‹ˆë‹¤!

1. **API ì„œë²„ê°€ ì´ë¯¸ êµ¬ì¶•ë˜ì–´ ìˆìŒ**
   - `src/api_server.py` (ì¶”ë¡ ìš©)
   - `src/training_api.py` (í•™ìŠµ ê´€ë¦¬ìš©)

2. **í´ë¼ì´ì–¸íŠ¸ ì§€ì›**
   - Python í´ë¼ì´ì–¸íŠ¸ ì œê³µ (`scripts/api_client.py`)
   - ëª¨ë“  ì–¸ì–´ì—ì„œ HTTPë¡œ ì ‘ê·¼ ê°€ëŠ¥

3. **ì‚¬ìš© ë°©ë²•**
   ```bash
   # ì„œë²„ ì‹œì‘
   python src/api_server.py --model_path "your-model"
   
   # í´ë¼ì´ì–¸íŠ¸ì—ì„œ ì‚¬ìš©
   curl http://localhost:8000/chat -d '{"instruction": "ì§ˆë¬¸"}'
   ```

### ğŸ“š ê´€ë ¨ ë¬¸ì„œ

- **API_GUIDE.md** - ì™„ì „í•œ API ê°€ì´ë“œ
- **QUICKSTART_API.md** - API ë¹ ë¥¸ ì‹œì‘
- **scripts/api_client.py** - Python í´ë¼ì´ì–¸íŠ¸
- **scripts/test_api.py** - í…ŒìŠ¤íŠ¸ ë„êµ¬

### ğŸ¯ ë°”ë¡œ ì‹œì‘í•˜ê¸°

```bash
# 1. API ì„œë²„ ì‹œì‘
python src/api_server.py \
    --model_path "outputs/checkpoints/final_model"

# 2. ë¸Œë¼ìš°ì €ì—ì„œ ì—´ê¸°
# http://localhost:8000/docs

# 3. Pythonìœ¼ë¡œ ì‚¬ìš©
python scripts/api_client.py
```

**ëª¨ë“  í´ë¼ì´ì–¸íŠ¸ì—ì„œ REST APIë¡œ ì ‘ê·¼ ê°€ëŠ¥í•©ë‹ˆë‹¤!** ğŸš€
