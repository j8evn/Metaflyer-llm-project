# vLLM í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš© ê°€ì´ë“œ

vLLMìœ¼ë¡œ ì„œë¹„ìŠ¤ë˜ëŠ” LLMì— ì ‘ê·¼í•˜ëŠ” ë‹¤ì–‘í•œ ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤.

## ëª©ì°¨
1. [vLLM ì„œë²„ ì‹œì‘](#vllm-ì„œë²„-ì‹œì‘)
2. [OpenAI Python ë¼ì´ë¸ŒëŸ¬ë¦¬](#1-openai-python-ë¼ì´ë¸ŒëŸ¬ë¦¬-ê¶Œì¥)
3. [HTTP ìš”ì²­](#2-http-ìš”ì²­-curlrequests)
4. [LangChain](#3-langchain)
5. [ê¸°íƒ€ í´ë¼ì´ì–¸íŠ¸](#4-ê¸°íƒ€-í´ë¼ì´ì–¸íŠ¸)

---

## vLLM ì„œë²„ ì‹œì‘

ë¨¼ì € vLLM ì„œë²„ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤:

### ì„¤ì¹˜

```bash
pip install vllm
```

### ê¸°ë³¸ ì„œë²„ ì‹œì‘

```bash
# ê¸°ë³¸ ëª¨ë¸
vllm serve meta-llama/Llama-2-7b-hf --port 8000

# íŒŒì¸íŠœë‹í•œ ëª¨ë¸
vllm serve outputs/my_model/final_model --port 8000

# ì—¬ëŸ¬ GPU ì‚¬ìš©
vllm serve meta-llama/Llama-2-7b-hf \
    --port 8000 \
    --tensor-parallel-size 2
```

ì„œë²„ê°€ ì‹œì‘ë˜ë©´:
- **OpenAI í˜¸í™˜ API**: http://localhost:8000/v1
- **ë¬¸ì„œ**: http://localhost:8000/docs

---

## 1. OpenAI Python ë¼ì´ë¸ŒëŸ¬ë¦¬ (ê¶Œì¥) â­â­â­â­â­

vLLMì€ OpenAI APIì™€ ì™„ì „íˆ í˜¸í™˜ë˜ë¯€ë¡œ, OpenAI ê³µì‹ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!

### ì„¤ì¹˜

```bash
pip install openai
```

### ê¸°ë³¸ ì‚¬ìš©

```python
from openai import OpenAI

# vLLM ì„œë²„ì— ì—°ê²° (base_urlë§Œ ë³€ê²½)
client = OpenAI(
    base_url="http://localhost:8000/v1",
    api_key="EMPTY"  # vLLMì€ API í‚¤ ë¶ˆí•„ìš”
)

# ì±„íŒ… ì™„ì„±
response = client.chat.completions.create(
    model="meta-llama/Llama-2-7b-hf",  # vLLMì—ì„œ ì‹¤í–‰ ì¤‘ì¸ ëª¨ë¸
    messages=[
        {"role": "user", "content": "Pythonì´ë€ ë¬´ì—‡ì¸ê°€ìš”?"}
    ],
    max_tokens=200,
    temperature=0.7
)

print(response.choices[0].message.content)
```

### í…ìŠ¤íŠ¸ ì™„ì„±

```python
from openai import OpenAI

client = OpenAI(
    base_url="http://localhost:8000/v1",
    api_key="EMPTY"
)

# í…ìŠ¤íŠ¸ ì™„ì„± (Completion)
response = client.completions.create(
    model="meta-llama/Llama-2-7b-hf",
    prompt="Python is a",
    max_tokens=100,
    temperature=0.7
)

print(response.choices[0].text)
```

### ìŠ¤íŠ¸ë¦¬ë°

```python
from openai import OpenAI

client = OpenAI(
    base_url="http://localhost:8000/v1",
    api_key="EMPTY"
)

# ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
stream = client.chat.completions.create(
    model="meta-llama/Llama-2-7b-hf",
    messages=[{"role": "user", "content": "ê¸´ ì´ì•¼ê¸°ë¥¼ ë“¤ë ¤ì£¼ì„¸ìš”"}],
    stream=True,
    max_tokens=500
)

for chunk in stream:
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end="", flush=True)
```

### ì™„ì „í•œ ì˜ˆì œ

```python
# vllm_client.py
from openai import OpenAI

class VLLMClient:
    """vLLM í´ë¼ì´ì–¸íŠ¸ ë˜í¼"""
    
    def __init__(self, base_url: str = "http://localhost:8000/v1"):
        self.client = OpenAI(
            base_url=base_url,
            api_key="EMPTY"
        )
    
    def chat(self, message: str, **kwargs):
        """ê°„ë‹¨í•œ ì±„íŒ…"""
        response = self.client.chat.completions.create(
            model=kwargs.get("model", "meta-llama/Llama-2-7b-hf"),
            messages=[{"role": "user", "content": message}],
            max_tokens=kwargs.get("max_tokens", 200),
            temperature=kwargs.get("temperature", 0.7)
        )
        return response.choices[0].message.content
    
    def stream_chat(self, message: str, **kwargs):
        """ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ…"""
        stream = self.client.chat.completions.create(
            model=kwargs.get("model", "meta-llama/Llama-2-7b-hf"),
            messages=[{"role": "user", "content": message}],
            stream=True,
            max_tokens=kwargs.get("max_tokens", 500),
            temperature=kwargs.get("temperature", 0.7)
        )
        
        for chunk in stream:
            if chunk.choices[0].delta.content:
                yield chunk.choices[0].delta.content

# ì‚¬ìš©
if __name__ == "__main__":
    client = VLLMClient()
    
    # ì¼ë°˜ ì±„íŒ…
    response = client.chat("Pythonì˜ ì¥ì ì€?")
    print(response)
    
    # ìŠ¤íŠ¸ë¦¬ë°
    print("\nìŠ¤íŠ¸ë¦¬ë°:")
    for chunk in client.stream_chat("ì¸ê³µì§€ëŠ¥ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”"):
        print(chunk, end="", flush=True)
```

---

## 2. HTTP ìš”ì²­ (curl/requests)

### cURL

```bash
# ì±„íŒ… ì™„ì„±
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "meta-llama/Llama-2-7b-hf",
    "messages": [
      {"role": "user", "content": "Pythonì´ë€?"}
    ],
    "max_tokens": 200,
    "temperature": 0.7
  }'

# í…ìŠ¤íŠ¸ ì™„ì„±
curl http://localhost:8000/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "meta-llama/Llama-2-7b-hf",
    "prompt": "Python is a",
    "max_tokens": 100
  }'
```

### Python requests

```python
import requests

# vLLM ì„œë²„ URL
BASE_URL = "http://localhost:8000/v1"

def chat(message: str):
    """ì±„íŒ… API í˜¸ì¶œ"""
    response = requests.post(
        f"{BASE_URL}/chat/completions",
        json={
            "model": "meta-llama/Llama-2-7b-hf",
            "messages": [
                {"role": "user", "content": message}
            ],
            "max_tokens": 200,
            "temperature": 0.7
        }
    )
    
    result = response.json()
    return result['choices'][0]['message']['content']

# ì‚¬ìš©
response = chat("Pythonì˜ ì¥ì ì€?")
print(response)
```

### JavaScript/TypeScript

```javascript
// Node.js
const fetch = require('node-fetch');

async function chat(message) {
  const response = await fetch('http://localhost:8000/v1/chat/completions', {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({
      model: 'meta-llama/Llama-2-7b-hf',
      messages: [
        { role: 'user', content: message }
      ],
      max_tokens: 200,
      temperature: 0.7
    })
  });
  
  const data = await response.json();
  return data.choices[0].message.content;
}

// ì‚¬ìš©
chat('Pythonì´ë€?').then(console.log);
```

---

## 3. LangChain

LangChainë„ vLLMê³¼ ì‰½ê²Œ í†µí•©ë©ë‹ˆë‹¤.

### ì„¤ì¹˜

```bash
pip install langchain langchain-openai
```

### ChatOpenAI ì‚¬ìš©

```python
from langchain_openai import ChatOpenAI

# vLLMì„ ChatOpenAIë¡œ ì‚¬ìš©
llm = ChatOpenAI(
    model="meta-llama/Llama-2-7b-hf",
    openai_api_key="EMPTY",
    openai_api_base="http://localhost:8000/v1",
    max_tokens=200,
    temperature=0.7
)

# ì§ì ‘ í˜¸ì¶œ
response = llm.invoke("Pythonì´ë€ ë¬´ì—‡ì¸ê°€ìš”?")
print(response.content)
```

### Chainê³¼ í•¨ê»˜ ì‚¬ìš©

```python
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema.output_parser import StrOutputParser

# LLM ì„¤ì •
llm = ChatOpenAI(
    model="meta-llama/Llama-2-7b-hf",
    openai_api_key="EMPTY",
    openai_api_base="http://localhost:8000/v1"
)

# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
prompt = ChatPromptTemplate.from_messages([
    ("system", "ë‹¹ì‹ ì€ ì¹œì ˆí•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."),
    ("user", "{question}")
])

# Chain êµ¬ì„±
chain = prompt | llm | StrOutputParser()

# ì‹¤í–‰
response = chain.invoke({"question": "Pythonì˜ ì¥ì ì€?"})
print(response)
```

### RAG (Retrieval Augmented Generation)

```python
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain.document_loaders import TextLoader
from langchain.text_splitter import CharacterTextSplitter

# vLLM LLM
llm = ChatOpenAI(
    model="meta-llama/Llama-2-7b-hf",
    openai_api_key="EMPTY",
    openai_api_base="http://localhost:8000/v1"
)

# ë¬¸ì„œ ë¡œë”©
loader = TextLoader("documents.txt")
documents = loader.load()

# ë¶„í• 
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
texts = text_splitter.split_documents(documents)

# ë²¡í„° ì €ì¥ì†Œ
embeddings = OpenAIEmbeddings()
vectorstore = FAISS.from_documents(texts, embeddings)

# RAG Chain
qa = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=vectorstore.as_retriever()
)

# ì§ˆì˜
response = qa.invoke("ë¬¸ì„œì˜ ì£¼ìš” ë‚´ìš©ì€?")
print(response)
```

---

## 4. ê¸°íƒ€ í´ë¼ì´ì–¸íŠ¸

### OpenAI JavaScript/TypeScript SDK

```bash
npm install openai
```

```typescript
import OpenAI from 'openai';

const client = new OpenAI({
  baseURL: 'http://localhost:8000/v1',
  apiKey: 'EMPTY'
});

async function chat(message: string) {
  const response = await client.chat.completions.create({
    model: 'meta-llama/Llama-2-7b-hf',
    messages: [{ role: 'user', content: message }],
    max_tokens: 200
  });
  
  return response.choices[0].message.content;
}

// ì‚¬ìš©
chat('Pythonì´ë€?').then(console.log);
```

### Go

```go
package main

import (
    "context"
    "fmt"
    openai "github.com/sashabaranov/go-openai"
)

func main() {
    config := openai.DefaultConfig("EMPTY")
    config.BaseURL = "http://localhost:8000/v1"
    client := openai.NewClientWithConfig(config)
    
    resp, err := client.CreateChatCompletion(
        context.Background(),
        openai.ChatCompletionRequest{
            Model: "meta-llama/Llama-2-7b-hf",
            Messages: []openai.ChatCompletionMessage{
                {
                    Role:    openai.ChatMessageRoleUser,
                    Content: "Pythonì´ë€?",
                },
            },
        },
    )
    
    if err != nil {
        fmt.Printf("Error: %v\n", err)
        return
    }
    
    fmt.Println(resp.Choices[0].Message.Content)
}
```

---

## ì‹¤ì „ í†µí•© ì˜ˆì œ

### ì´ í”„ë¡œì íŠ¸ + vLLM í†µí•©

```python
# integrated_client.py
"""
ì´ í”„ë¡œì íŠ¸ë¡œ íŒŒì¸íŠœë‹í•œ ëª¨ë¸ì„ vLLMìœ¼ë¡œ ì„œë¹„ìŠ¤
"""

from openai import OpenAI

class FinetunedLLMClient:
    """íŒŒì¸íŠœë‹ëœ ëª¨ë¸ í´ë¼ì´ì–¸íŠ¸"""
    
    def __init__(
        self,
        model_path: str,
        vllm_base_url: str = "http://localhost:8000/v1"
    ):
        self.model_path = model_path
        self.client = OpenAI(
            base_url=vllm_base_url,
            api_key="EMPTY"
        )
    
    def chat(self, instruction: str, input_text: str = ""):
        """Instruction í˜•ì‹ìœ¼ë¡œ ì§ˆì˜"""
        
        # Instruction í˜•ì‹ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        if input_text:
            prompt = f"""### Instruction:
{instruction}

### Input:
{input_text}

### Response:
"""
        else:
            prompt = f"""### Instruction:
{instruction}

### Response:
"""
        
        # vLLM í˜¸ì¶œ
        response = self.client.completions.create(
            model=self.model_path,
            prompt=prompt,
            max_tokens=256,
            temperature=0.7,
            stop=["###"]  # Instruction êµ¬ë¶„ìì—ì„œ ì¤‘ì§€
        )
        
        return response.choices[0].text.strip()
    
    def batch_process(self, questions: list):
        """ë°°ì¹˜ ì²˜ë¦¬"""
        results = []
        for q in questions:
            result = self.chat(q)
            results.append(result)
        return results

# ì‚¬ìš©
if __name__ == "__main__":
    # 1. vLLM ì„œë²„ ì‹œì‘ (ë³„ë„ í„°ë¯¸ë„)
    # vllm serve outputs/my_model/final_model --port 8000
    
    # 2. í´ë¼ì´ì–¸íŠ¸ ìƒì„±
    client = FinetunedLLMClient(
        model_path="outputs/my_model/final_model"
    )
    
    # 3. ì‚¬ìš©
    response = client.chat(
        instruction="Pythonì˜ ì¥ì ì„ ì„¤ëª…í•˜ì„¸ìš”"
    )
    print(response)
```

### ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ í†µí•©

```python
# app.py (FastAPI + vLLM)
from fastapi import FastAPI
from pydantic import BaseModel
from openai import OpenAI

app = FastAPI()

# vLLM í´ë¼ì´ì–¸íŠ¸
vllm_client = OpenAI(
    base_url="http://localhost:8000/v1",
    api_key="EMPTY"
)

class Question(BaseModel):
    text: str

@app.post("/ask")
async def ask_question(question: Question):
    """ì‚¬ìš©ì ì§ˆë¬¸ ì²˜ë¦¬"""
    response = vllm_client.chat.completions.create(
        model="meta-llama/Llama-2-7b-hf",
        messages=[
            {"role": "user", "content": question.text}
        ],
        max_tokens=200
    )
    
    return {
        "question": question.text,
        "answer": response.choices[0].message.content
    }

# ì‹¤í–‰: uvicorn app:app --port 8080
```

---

## ì„±ëŠ¥ ìµœì í™”

### 1. ë°°ì¹˜ ì²˜ë¦¬

```python
from openai import OpenAI

client = OpenAI(
    base_url="http://localhost:8000/v1",
    api_key="EMPTY"
)

# ì—¬ëŸ¬ ìš”ì²­ì„ ë™ì‹œì—
questions = [
    "Pythonì´ë€?",
    "ë¨¸ì‹ ëŸ¬ë‹ì´ë€?",
    "ë”¥ëŸ¬ë‹ì´ë€?"
]

responses = []
for q in questions:
    response = client.chat.completions.create(
        model="meta-llama/Llama-2-7b-hf",
        messages=[{"role": "user", "content": q}],
        max_tokens=100
    )
    responses.append(response.choices[0].message.content)
```

### 2. ë¹„ë™ê¸° ì²˜ë¦¬

```python
import asyncio
from openai import AsyncOpenAI

async def ask_question(client, question):
    response = await client.chat.completions.create(
        model="meta-llama/Llama-2-7b-hf",
        messages=[{"role": "user", "content": question}],
        max_tokens=100
    )
    return response.choices[0].message.content

async def main():
    client = AsyncOpenAI(
        base_url="http://localhost:8000/v1",
        api_key="EMPTY"
    )
    
    questions = [
        "Pythonì´ë€?",
        "ë¨¸ì‹ ëŸ¬ë‹ì´ë€?",
        "ë”¥ëŸ¬ë‹ì´ë€?"
    ]
    
    # ë™ì‹œ ì‹¤í–‰
    tasks = [ask_question(client, q) for q in questions]
    responses = await asyncio.gather(*tasks)
    
    for q, r in zip(questions, responses):
        print(f"Q: {q}")
        print(f"A: {r}\n")

# ì‹¤í–‰
asyncio.run(main())
```

---

## íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### ì—°ê²° ì˜¤ë¥˜

```python
import requests

# vLLM ì„œë²„ í™•ì¸
try:
    response = requests.get("http://localhost:8000/health")
    print("ì„œë²„ ì •ìƒ:", response.json())
except:
    print("ì„œë²„ ì—°ê²° ì‹¤íŒ¨. vLLM ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”.")
```

### ëª¨ë¸ ì´ë¦„ í™•ì¸

```python
from openai import OpenAI

client = OpenAI(
    base_url="http://localhost:8000/v1",
    api_key="EMPTY"
)

# ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡
models = client.models.list()
print("ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸:")
for model in models.data:
    print(f"  - {model.id}")
```

---

## ìš”ì•½

### ì¶”ì²œ ìˆœì„œ

1. **OpenAI Python ë¼ì´ë¸ŒëŸ¬ë¦¬** â­â­â­â­â­
   - ê°€ì¥ ì‰½ê³  ê°•ë ¥í•¨
   - ê³µì‹ ì§€ì›
   
2. **LangChain** â­â­â­â­
   - ë³µì¡í•œ ì›Œí¬í”Œë¡œìš°
   - RAG, Chain ë“±
   
3. **HTTP ì§ì ‘ í˜¸ì¶œ** â­â­â­
   - ê°„ë‹¨í•œ ìš”ì²­
   - ë‹¤ë¥¸ ì–¸ì–´

### ë¹ ë¥¸ ì‹œì‘

```bash
# 1. vLLM ì„¤ì¹˜ ë° ì‹œì‘
pip install vllm
vllm serve meta-llama/Llama-2-7b-hf --port 8000

# 2. OpenAI ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
pip install openai

# 3. Pythonì—ì„œ ì‚¬ìš©
python
>>> from openai import OpenAI
>>> client = OpenAI(base_url="http://localhost:8000/v1", api_key="EMPTY")
>>> response = client.chat.completions.create(
...     model="meta-llama/Llama-2-7b-hf",
...     messages=[{"role": "user", "content": "Hello!"}]
... )
>>> print(response.choices[0].message.content)
```

ëª¨ë“  OpenAI í˜¸í™˜ í´ë¼ì´ì–¸íŠ¸ê°€ ì‘ë™í•©ë‹ˆë‹¤! ğŸš€

