# ìƒˆ ëª¨ë¸ ì¶”ê°€í•˜ê¸° - ì‹¤ì „ ê°€ì´ë“œ

ìƒˆë¡œìš´ LLM ëª¨ë¸ì„ í”„ë¡œì íŠ¸ì— ì¶”ê°€í•˜ëŠ” ì‹¤ì „ ê°€ì´ë“œì…ë‹ˆë‹¤.

## ğŸ¯ 90%ì˜ ê²½ìš°: ì•„ë¬´ê²ƒë„ ì•ˆ í•´ë„ ë©ë‹ˆë‹¤!

ëŒ€ë¶€ë¶„ì˜ Hugging Face ëª¨ë¸ì€ **ì¦‰ì‹œ ì‚¬ìš© ê°€ëŠ¥**í•©ë‹ˆë‹¤:

```bash
python src/train.py --model_name "ìƒˆë¡œìš´-ëª¨ë¸-ì´ë¦„"
```

ë! ğŸ‰

---

## ğŸ“ 3ë‹¨ê³„ë¡œ ìƒˆ ëª¨ë¸ ì¶”ê°€í•˜ê¸°

### 1ë‹¨ê³„: ëª¨ë¸ í˜¸í™˜ì„± í™•ì¸ (1ë¶„)

```bash
# ë¹ ë¥¸ ì²´í¬ (í† í¬ë‚˜ì´ì €ë§Œ)
python scripts/check_model_compatibility.py "mistralai/Mistral-7B-v0.1" --quick

# ì™„ì „í•œ ì²´í¬ (ëª¨ë¸ ë¡œë”© í¬í•¨)
python scripts/check_model_compatibility.py "mistralai/Mistral-7B-v0.1"
```

### 2ë‹¨ê³„: í…ŒìŠ¤íŠ¸ í•™ìŠµ (5-10ë¶„)

```bash
python src/train.py \
    --model_name "mistralai/Mistral-7B-v0.1" \
    --dataset_path "data/train.json" \
    --output_dir "outputs/test_mistral" \
    --num_epochs 1 \
    --batch_size 2
```

### 3ë‹¨ê³„: ì¶”ë¡  í…ŒìŠ¤íŠ¸ (1ë¶„)

```bash
python src/inference.py \
    --model_path "outputs/test_mistral/final_model" \
    --instruction "í…ŒìŠ¤íŠ¸ ì§ˆë¬¸"
```

---

## ğŸ”§ íŠ¹ìˆ˜ ì„¤ì •ì´ í•„ìš”í•œ ëª¨ë¸

### Qwen ëª¨ë¸

```yaml
# configs/train_config.yaml
model:
  name: "Qwen/Qwen-7B"
  trust_remote_code: true  # í•„ìˆ˜!
```

ì‹¤í–‰:
```bash
python src/train.py --config configs/train_config.yaml
```

### Gemma ëª¨ë¸

```bash
# 1. Hugging Face ë¡œê·¸ì¸ (í•„ìˆ˜)
huggingface-cli login

# 2. ëª¨ë¸ í˜ì´ì§€ì—ì„œ ë¼ì´ì„ ìŠ¤ ë™ì˜
# https://huggingface.co/google/gemma-7b

# 3. í•™ìŠµ
python src/train.py --model_name "google/gemma-7b"
```

### Falcon ëª¨ë¸

```yaml
# configs/train_config.yaml
model:
  name: "tiiuae/falcon-7b"
  trust_remote_code: true  # í•„ìˆ˜!
```

---

## ğŸ“‹ ì§€ì› ëª¨ë¸ ì „ì²´ ëª©ë¡

### ì¦‰ì‹œ ì‚¬ìš© ê°€ëŠ¥ (50+ ëª¨ë¸)

#### 7B í´ë˜ìŠ¤
- `meta-llama/Llama-2-7b-hf` - Meta AI
- `mistralai/Mistral-7B-v0.1` - Mistral AI
- `google/gemma-7b` - Google (ì¸ì¦ í•„ìš”)
- `Qwen/Qwen-7B` - Alibaba (trust_remote_code)
- `tiiuae/falcon-7b` - TII (trust_remote_code)
- `01-ai/Yi-6B` - 01.AI
- `stabilityai/stablelm-3b-4e1t` - Stability AI

#### ì†Œí˜• ëª¨ë¸ (í…ŒìŠ¤íŠ¸ìš©)
- `gpt2` - 124M (ê°€ì¥ ë¹ ë¦„)
- `gpt2-medium` - 355M
- `gpt2-large` - 774M
- `gpt2-xl` - 1.5B
- `microsoft/phi-2` - 2.7B

#### ëŒ€í˜• ëª¨ë¸
- `meta-llama/Llama-2-13b-hf` - 13B
- `meta-llama/Llama-2-70b-hf` - 70B
- `mistralai/Mixtral-8x7B-v0.1` - 8x7B MoE

### ì‚¬ìš© ì˜ˆì œ

```bash
# GPT-2 (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸)
python src/train.py --model_name "gpt2"

# Mistral 7B
python src/train.py --model_name "mistralai/Mistral-7B-v0.1"

# Qwen 7B
python src/train.py \
    --model_name "Qwen/Qwen-7B" \
    # configs/train_config.yamlì—ì„œ trust_remote_code: true ì„¤ì •

# Gemma 7B
huggingface-cli login
python src/train.py --model_name "google/gemma-7b"
```

---

## ğŸš€ ì‹¤ì „: ìƒˆ ëª¨ë¸ ì¶”ê°€ ì›Œí¬í”Œë¡œìš°

### ì˜ˆì‹œ: Yi-34B ëª¨ë¸ ì¶”ê°€

```bash
# 1. í˜¸í™˜ì„± í™•ì¸
python scripts/check_model_compatibility.py "01-ai/Yi-34B" --quick

# 2. ì„¤ì • íŒŒì¼ ìƒì„±
cat > configs/yi_config.yaml << 'YAML'
model:
  name: "01-ai/Yi-34B"

data:
  train_path: "data/train.json"
  max_length: 4096

lora:
  use_lora: true
  r: 16
  lora_alpha: 32

quantization:
  use_quantization: true  # 34B ëª¨ë¸ì€ ì–‘ìí™” ê¶Œì¥
  bits: 4

training:
  num_epochs: 3
  batch_size: 1  # í° ëª¨ë¸ì€ ì‘ì€ ë°°ì¹˜
  gradient_accumulation_steps: 16
YAML

# 3. í•™ìŠµ ì‹¤í–‰
python src/train.py --config configs/yi_config.yaml

# 4. ì¶”ë¡  í…ŒìŠ¤íŠ¸
python src/inference.py \
    --model_path "outputs/checkpoints/final_model" \
    --load_in_4bit
```

---

## ğŸ” ëª¨ë¸ ì„ íƒ ê°€ì´ë“œ

### ëª¨ë¸ í¬ê¸°ë³„ ì¶”ì²œ

| í¬ê¸° | ëª¨ë¸ | ìš©ë„ | GPU ë©”ëª¨ë¦¬ (LoRA) |
|------|------|------|-------------------|
| ~1B | GPT-2, Phi-2 | í…ŒìŠ¤íŠ¸, ì‹¤í—˜ | 4GB |
| 3-7B | Mistral-7B, Gemma-7B | ì¼ë°˜ ìš©ë„ | 16GB |
| 13B | Llama-2-13B | ê³ ì„±ëŠ¥ | 24GB |
| 34B+ | Yi-34B, Mixtral | ìµœê³  ì„±ëŠ¥ | 40GB+ (4bit) |

### ë¼ì´ì„ ìŠ¤ë³„ ë¶„ë¥˜

**ìƒì—…ì  ì‚¬ìš© ê°€ëŠ¥:**
- âœ… Mistral (Apache 2.0)
- âœ… Falcon (Apache 2.0)
- âœ… Yi (Apache 2.0)
- âœ… GPT-2 (MIT)

**ì œí•œì  ë¼ì´ì„ ìŠ¤:**
- âš ï¸ Llama 2 (Llama 2 Community License)
- âš ï¸ Gemma (Gemma Terms of Use)
- âš ï¸ Qwen (Tongyi Qianwen License)

---

## ğŸ’» ì½”ë“œ ì˜ˆì œ

### ì—¬ëŸ¬ ëª¨ë¸ë¡œ ìë™ ë²¤ì¹˜ë§ˆí¬

```python
# scripts/benchmark_models.py
"""
ì—¬ëŸ¬ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ
"""

import subprocess
import json

# í…ŒìŠ¤íŠ¸í•  ëª¨ë¸ ëª©ë¡
MODELS = [
    {"name": "gpt2", "batch_size": 8},
    {"name": "microsoft/phi-2", "batch_size": 4},
    {"name": "mistralai/Mistral-7B-v0.1", "batch_size": 4},
]

results = []

for model_info in MODELS:
    model_name = model_info["name"]
    batch_size = model_info["batch_size"]
    
    print(f"\n{'='*60}")
    print(f"ëª¨ë¸: {model_name}")
    print('='*60)
    
    # í•™ìŠµ
    cmd = [
        "python", "src/train.py",
        "--model_name", model_name,
        "--dataset_path", "data/train.json",
        "--output_dir", f"outputs/benchmark_{model_name.split('/')[-1]}",
        "--num_epochs", "1",
        "--batch_size", str(batch_size),
        "--use_lora"
    ]
    
    subprocess.run(cmd)
    
    # í‰ê°€
    subprocess.run([
        "python", "scripts/evaluate_model.py",
        "--model_path", f"outputs/benchmark_{model_name.split('/')[-1]}",
        "--eval_data", "data/eval.json",
        "--output_path", f"benchmark_{model_name.split('/')[-1]}.json"
    ])

print("\në²¤ì¹˜ë§ˆí¬ ì™„ë£Œ!")
```

### ëª¨ë¸ ìë™ ì„ íƒê¸°

```python
# src/model_selector.py (ìƒˆ íŒŒì¼)
"""
ì‘ì—…ì— ë§ëŠ” ëª¨ë¸ ìë™ ì„ íƒ
"""

import torch

def select_model_for_task(
    task: str,
    available_memory_gb: float = None
) -> str:
    """
    ì‘ì—…ê³¼ í•˜ë“œì›¨ì–´ì— ë§ëŠ” ëª¨ë¸ ì„ íƒ
    
    Args:
        task: 'general', 'code', 'translation', 'chat' ë“±
        available_memory_gb: ì‚¬ìš© ê°€ëŠ¥í•œ GPU ë©”ëª¨ë¦¬ (GB)
    """
    
    # GPU ë©”ëª¨ë¦¬ ìë™ ê°ì§€
    if available_memory_gb is None and torch.cuda.is_available():
        available_memory_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3
    
    # ë©”ëª¨ë¦¬ë³„ ì¶”ì²œ
    if available_memory_gb is None or available_memory_gb < 8:
        # CPU ë˜ëŠ” ì‘ì€ GPU
        return "gpt2"
    
    elif available_memory_gb < 16:
        # 8-16GB GPU
        return "microsoft/phi-2"
    
    elif available_memory_gb < 24:
        # 16-24GB GPU
        if task == "code":
            return "codellama/CodeLlama-7b-hf"
        else:
            return "mistralai/Mistral-7B-v0.1"
    
    else:
        # 24GB+ GPU
        if task == "code":
            return "codellama/CodeLlama-13b-hf"
        elif task == "translation":
            return "facebook/nllb-200-3.3B"
        else:
            return "meta-llama/Llama-2-13b-hf"

# ì‚¬ìš©
if __name__ == "__main__":
    recommended_model = select_model_for_task("general")
    print(f"ì¶”ì²œ ëª¨ë¸: {recommended_model}")
```

---

## ğŸ“ ëª¨ë¸ ì¶”ê°€ ì²´í¬ë¦¬ìŠ¤íŠ¸

ìƒˆ ëª¨ë¸ì„ ì¶”ê°€í•  ë•Œ í™•ì¸í•  ì‚¬í•­:

### âœ… í•„ìˆ˜ í™•ì¸
- [ ] Hugging Faceì—ì„œ ëª¨ë¸ í˜ì´ì§€ í™•ì¸
- [ ] ë¼ì´ì„ ìŠ¤ í™•ì¸ (ìƒì—…ì  ì‚¬ìš© ê°€ëŠ¥í•œì§€)
- [ ] í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸ ì‹¤í–‰
- [ ] í…ŒìŠ¤íŠ¸ í•™ìŠµ ì‹¤í–‰ (1 ì—í¬í¬)
- [ ] ì¶”ë¡  í…ŒìŠ¤íŠ¸

### âœ… ì„ íƒ í™•ì¸
- [ ] ìµœì  LoRA ì„¤ì • ì°¾ê¸°
- [ ] ë°°ì¹˜ í¬ê¸° ì¡°ì •
- [ ] `supported_models.yaml`ì— ì¶”ê°€
- [ ] README.md ì—…ë°ì´íŠ¸

### âœ… íŠ¹ìˆ˜ ìš”êµ¬ì‚¬í•­
- [ ] `trust_remote_code` í•„ìš” ì—¬ë¶€
- [ ] ì¸ì¦ í•„ìš” ì—¬ë¶€ (Hugging Face ë¡œê·¸ì¸)
- [ ] íŠ¹ìˆ˜ í† í¬ë‚˜ì´ì € ì„¤ì •

---

## ğŸ“Š ëª¨ë¸ ë¹„êµí‘œ

### ì¸ê¸° ëª¨ë¸ ë¹„êµ

| ëª¨ë¸ | í¬ê¸° | ì†ë„ | í’ˆì§ˆ | ìƒì—…ìš© | íŠ¹ì´ì‚¬í•­ |
|------|------|------|------|--------|----------|
| GPT-2 | 124M | â­â­â­â­â­ | â­â­ | âœ… | í…ŒìŠ¤íŠ¸ìš© |
| Phi-2 | 2.7B | â­â­â­â­ | â­â­â­ | âœ… | ì‘ì§€ë§Œ ê°•ë ¥ |
| Mistral-7B | 7B | â­â­â­ | â­â­â­â­ | âœ… | ê· í˜•ì¡íŒ |
| Llama-2-7B | 7B | â­â­â­ | â­â­â­â­ | âš ï¸ | ì¸ê¸° ë§ìŒ |
| Gemma-7B | 7B | â­â­â­ | â­â­â­â­ | âš ï¸ | Google ìµœì‹  |
| Qwen-7B | 7B | â­â­â­ | â­â­â­â­ | âœ… | ë‹¤êµ­ì–´ ê°•í•¨ |

---

## ğŸ› ï¸ ê³ ê¸‰: ì™„ì „íˆ ìƒˆë¡œìš´ ì•„í‚¤í…ì²˜

ì™„ì „íˆ ìƒˆë¡œìš´ ëª¨ë¸ ì•„í‚¤í…ì²˜ë¥¼ ì¶”ê°€í•˜ë ¤ë©´:

### 1. ì»¤ìŠ¤í…€ ëª¨ë¸ í´ë˜ìŠ¤ ì •ì˜

```python
# src/custom_architecture.py
from transformers import PreTrainedModel, PretrainedConfig

class MyCustomConfig(PretrainedConfig):
    model_type = "my_custom_model"
    
    def __init__(self, vocab_size=50000, hidden_size=768, **kwargs):
        super().__init__(**kwargs)
        self.vocab_size = vocab_size
        self.hidden_size = hidden_size

class MyCustomModel(PreTrainedModel):
    config_class = MyCustomConfig
    
    def __init__(self, config):
        super().__init__(config)
        # ëª¨ë¸ ë ˆì´ì–´ ì •ì˜
        ...
    
    def forward(self, input_ids, **kwargs):
        # ìˆœì „íŒŒ
        ...

# ë“±ë¡
from transformers import AutoConfig, AutoModelForCausalLM
AutoConfig.register("my_custom_model", MyCustomConfig)
AutoModelForCausalLM.register(MyCustomConfig, MyCustomModel)
```

### 2. train.pyì—ì„œ ì„í¬íŠ¸

```python
# src/train.py ìƒë‹¨ì— ì¶”ê°€
try:
    from custom_architecture import MyCustomModel
except ImportError:
    pass
```

---

## ğŸ¯ ì‹¤ì „ ì‹œë‚˜ë¦¬ì˜¤

### ì‹œë‚˜ë¦¬ì˜¤ 1: "CodeLlamaë¥¼ ì¶”ê°€í•˜ê³  ì‹¶ì–´ìš”"

```bash
# ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥!
python src/train.py \
    --model_name "codellama/CodeLlama-7b-hf" \
    --dataset_path "data/code_dataset.json" \
    --use_lora
```

### ì‹œë‚˜ë¦¬ì˜¤ 2: "íšŒì‚¬ ìì²´ ëª¨ë¸ì„ ì‚¬ìš©í•˜ê³  ì‹¶ì–´ìš”"

```bash
# ë¡œì»¬ ëª¨ë¸ ê²½ë¡œ ì‚¬ìš©
python src/train.py \
    --model_name "/path/to/company/model" \
    --dataset_path "data/company_data.json"
```

### ì‹œë‚˜ë¦¬ì˜¤ 3: "ìµœì‹  SOTA ëª¨ë¸ì„ í…ŒìŠ¤íŠ¸í•˜ê³  ì‹¶ì–´ìš”"

```bash
# 1. í˜¸í™˜ì„± ë¨¼ì € ì²´í¬
python scripts/check_model_compatibility.py "new-model/sota-7b" --quick

# 2. ì‘ì€ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸
python src/train.py \
    --model_name "new-model/sota-7b" \
    --dataset_path "data/train.json" \
    --num_epochs 1

# 3. ì„±ê³µí•˜ë©´ ë³¸ê²© í•™ìŠµ
python src/train.py --config configs/train_config.yaml
```

---

## ğŸ“š ì°¸ê³  ìë£Œ

- **Hugging Face Models**: https://huggingface.co/models
- **ì§€ì› ëª¨ë¸ ëª©ë¡**: `configs/supported_models.yaml`
- **í˜¸í™˜ì„± ì²´í¬**: `scripts/check_model_compatibility.py`
- **ìƒì„¸ ê°€ì´ë“œ**: `MODEL_EXTENSION_GUIDE.md`

---

## ìš”ì•½

### ëŒ€ë¶€ë¶„ì˜ ê²½ìš°

```bash
# ê·¸ëƒ¥ ëª¨ë¸ ì´ë¦„ë§Œ ë°”ê¾¸ë©´ ë©ë‹ˆë‹¤!
python src/train.py --model_name "ì›í•˜ëŠ”-ëª¨ë¸"
```

### íŠ¹ìˆ˜í•œ ê²½ìš°ë§Œ

- Qwen, Falcon â†’ `trust_remote_code: true`
- Gemma â†’ `huggingface-cli login`
- í° ëª¨ë¸ â†’ `quantization` í™œì„±í™”

**50ê°œ ì´ìƒì˜ ëª¨ë¸ì´ ì¦‰ì‹œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤!** ğŸ‰
