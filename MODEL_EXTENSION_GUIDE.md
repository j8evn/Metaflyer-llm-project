# ì§€ì› ëª¨ë¸ í™•ì¥ ê°€ì´ë“œ

ì´ í”„ë¡œì íŠ¸ì— ìƒˆë¡œìš´ LLM ëª¨ë¸ì„ ì¶”ê°€í•˜ê³  ì§€ì›í•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤.

## ëª©ì°¨
1. [í˜„ì¬ ì§€ì› ëª¨ë¸](#í˜„ì¬-ì§€ì›-ëª¨ë¸)
2. [ìƒˆ Hugging Face ëª¨ë¸ ì¶”ê°€](#ìƒˆ-hugging-face-ëª¨ë¸-ì¶”ê°€)
3. [ì»¤ìŠ¤í…€ ì•„í‚¤í…ì²˜ ì§€ì›](#ì»¤ìŠ¤í…€-ì•„í‚¤í…ì²˜-ì§€ì›)
4. [ë‹¤ë¥¸ í˜•ì‹ ëª¨ë¸ í†µí•©](#ë‹¤ë¥¸-í˜•ì‹-ëª¨ë¸-í†µí•©)
5. [ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ ì§€ì›](#ë©€í‹°ëª¨ë‹¬-ëª¨ë¸-ì§€ì›)

---

## í˜„ì¬ ì§€ì› ëª¨ë¸

### ì¦‰ì‹œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ë“¤

ì´ í”„ë¡œì íŠ¸ëŠ” Hugging Face Transformers ê¸°ë°˜ì´ë¯€ë¡œ, **ëŒ€ë¶€ë¶„ì˜ Causal LM ëª¨ë¸**ì„ ì¦‰ì‹œ ì§€ì›í•©ë‹ˆë‹¤:

#### í…ìŠ¤íŠ¸ ìƒì„± ëª¨ë¸
- âœ… **Llama 2/3** (Meta)
- âœ… **Mistral/Mixtral** (Mistral AI)
- âœ… **Falcon** (TII)
- âœ… **GPT-2/GPT-J/GPT-Neo** (EleutherAI)
- âœ… **BLOOM** (BigScience)
- âœ… **Qwen** (Alibaba)
- âœ… **Yi** (01.AI)
- âœ… **Gemma** (Google)
- âœ… **Phi** (Microsoft)
- âœ… **StableLM** (Stability AI)

#### ì‚¬ìš© ë°©ë²•

ëª¨ë¸ ì´ë¦„ë§Œ ë³€ê²½í•˜ë©´ ë©ë‹ˆë‹¤:

```bash
# Llama 2
python src/train.py --model_name "meta-llama/Llama-2-7b-hf"

# Mistral
python src/train.py --model_name "mistralai/Mistral-7B-v0.1"

# Qwen
python src/train.py --model_name "Qwen/Qwen-7B"

# Gemma
python src/train.py --model_name "google/gemma-7b"
```

---

## ìƒˆ Hugging Face ëª¨ë¸ ì¶”ê°€

### ë°©ë²• 1: ëª¨ë¸ ì´ë¦„ìœ¼ë¡œ ì§ì ‘ ì‚¬ìš©

ëŒ€ë¶€ë¶„ì˜ ê²½ìš° **ì¶”ê°€ ì‘ì—… ì—†ì´** ëª¨ë¸ ì´ë¦„ë§Œìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.

```bash
# 1. ëª¨ë¸ ê²€ìƒ‰
# https://huggingface.co/models ì—ì„œ ê²€ìƒ‰

# 2. ëª¨ë¸ ID ë³µì‚¬
# ì˜ˆ: "upstage/SOLAR-10.7B-v1.0"

# 3. ë°”ë¡œ ì‚¬ìš©
python src/train.py \
    --model_name "upstage/SOLAR-10.7B-v1.0" \
    --dataset_path "data/train.json" \
    --use_lora
```

### ë°©ë²• 2: ëª¨ë¸ ì„¤ì • íŒŒì¼ì— ì¶”ê°€

ì—¬ëŸ¬ ëª¨ë¸ì„ ê´€ë¦¬í•˜ë ¤ë©´ ì„¤ì • íŒŒì¼ì„ ì‚¬ìš©:

```yaml
# configs/models.yaml
models:
  llama2-7b:
    name: "meta-llama/Llama-2-7b-hf"
    context_length: 4096
    recommended_batch_size: 4
    
  mistral-7b:
    name: "mistralai/Mistral-7B-v0.1"
    context_length: 8192
    recommended_batch_size: 4
    
  qwen-7b:
    name: "Qwen/Qwen-7B"
    context_length: 8192
    recommended_batch_size: 4
    trust_remote_code: true  # Qwenì€ trust_remote_code í•„ìš”
    
  gemma-7b:
    name: "google/gemma-7b"
    context_length: 8192
    recommended_batch_size: 4
```

### ë°©ë²• 3: ëª¨ë¸ë³„ LoRA íƒ€ê²Ÿ ì»¤ìŠ¤í„°ë§ˆì´ì§•

ëª¨ë¸ë§ˆë‹¤ ìµœì ì˜ LoRA íƒ€ê²Ÿ ëª¨ë“ˆì´ ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
# src/model_configs.py (ìƒˆ íŒŒì¼ ìƒì„±)
"""
ëª¨ë¸ë³„ ìµœì  ì„¤ì •
"""

MODEL_CONFIGS = {
    # Llama ê³„ì—´
    "llama": {
        "target_modules": [
            "q_proj", "v_proj", "k_proj", "o_proj",
            "gate_proj", "up_proj", "down_proj"
        ],
        "lora_r": 16,
        "lora_alpha": 32
    },
    
    # Mistral ê³„ì—´
    "mistral": {
        "target_modules": [
            "q_proj", "v_proj", "k_proj", "o_proj",
            "gate_proj", "up_proj", "down_proj"
        ],
        "lora_r": 16,
        "lora_alpha": 32
    },
    
    # GPT-2 ê³„ì—´
    "gpt2": {
        "target_modules": [
            "c_attn", "c_proj", "c_fc"
        ],
        "lora_r": 8,
        "lora_alpha": 16
    },
    
    # Qwen ê³„ì—´
    "qwen": {
        "target_modules": [
            "c_attn", "c_proj", "w1", "w2"
        ],
        "lora_r": 16,
        "lora_alpha": 32
    },
    
    # Gemma ê³„ì—´
    "gemma": {
        "target_modules": [
            "q_proj", "v_proj", "k_proj", "o_proj",
            "gate_proj", "up_proj", "down_proj"
        ],
        "lora_r": 16,
        "lora_alpha": 32
    }
}

def get_model_config(model_name: str) -> dict:
    """ëª¨ë¸ ì´ë¦„ì—ì„œ ì„¤ì • ê°€ì ¸ì˜¤ê¸°"""
    model_name_lower = model_name.lower()
    
    for key, config in MODEL_CONFIGS.items():
        if key in model_name_lower:
            return config
    
    # ê¸°ë³¸ ì„¤ì • (Llama ìŠ¤íƒ€ì¼)
    return MODEL_CONFIGS["llama"]
```

### ë°©ë²• 4: model_utils.py í™•ì¥

ëª¨ë¸ë³„ íŠ¹ìˆ˜ ì²˜ë¦¬ê°€ í•„ìš”í•œ ê²½ìš°:

```python
# src/model_utils.pyì— ì¶”ê°€

def load_model(self, model_name: str) -> PreTrainedModel:
    """ëª¨ë¸ ë¡œë”© (í™•ì¥ ë²„ì „)"""
    logger.info(f"ëª¨ë¸ ë¡œë”©: {model_name}")
    
    # ëª¨ë¸ë³„ íŠ¹ìˆ˜ ì„¤ì •
    model_kwargs = self._get_model_kwargs(model_name)
    
    # ëª¨ë¸ ë¡œë”©
    model = AutoModelForCausalLM.from_pretrained(**model_kwargs)
    
    # ëª¨ë¸ë³„ í›„ì²˜ë¦¬
    model = self._post_process_model(model, model_name)
    
    return model

def _get_model_kwargs(self, model_name: str) -> dict:
    """ëª¨ë¸ë³„ ë¡œë”© ì¸ì"""
    kwargs = {
        'pretrained_model_name_or_path': model_name,
        'device_map': 'auto' if self.device == 'cuda' else None,
    }
    
    # Qwen: trust_remote_code í•„ìš”
    if 'qwen' in model_name.lower():
        kwargs['trust_remote_code'] = True
    
    # Falcon: trust_remote_code í•„ìš”
    if 'falcon' in model_name.lower():
        kwargs['trust_remote_code'] = True
    
    # Gemma: torch_dtype ì„¤ì •
    if 'gemma' in model_name.lower():
        kwargs['torch_dtype'] = torch.bfloat16
    
    # ì–‘ìí™” ì„¤ì • ì¶”ê°€
    quantization_config = self.get_quantization_config()
    if quantization_config:
        kwargs['quantization_config'] = quantization_config
    
    return kwargs

def _post_process_model(self, model, model_name: str):
    """ëª¨ë¸ í›„ì²˜ë¦¬"""
    # íŠ¹ì • ëª¨ë¸ì˜ íŠ¹ìˆ˜ ì²˜ë¦¬
    if 'mpt' in model_name.lower():
        # MPT ëª¨ë¸ íŠ¹ìˆ˜ ì²˜ë¦¬
        model.config.attn_config['attn_impl'] = 'torch'
    
    return model
```

---

## ì»¤ìŠ¤í…€ ì•„í‚¤í…ì²˜ ì§€ì›

### ì™„ì „íˆ ìƒˆë¡œìš´ ëª¨ë¸ ì•„í‚¤í…ì²˜ ì¶”ê°€

```python
# src/custom_models.py (ìƒˆ íŒŒì¼)
"""
ì»¤ìŠ¤í…€ ëª¨ë¸ ì•„í‚¤í…ì²˜
"""

import torch
import torch.nn as nn
from transformers import PreTrainedModel, PretrainedConfig

class CustomModelConfig(PretrainedConfig):
    """ì»¤ìŠ¤í…€ ëª¨ë¸ ì„¤ì •"""
    model_type = "custom_model"
    
    def __init__(
        self,
        vocab_size=32000,
        hidden_size=4096,
        num_hidden_layers=32,
        num_attention_heads=32,
        intermediate_size=11008,
        **kwargs
    ):
        super().__init__(**kwargs)
        self.vocab_size = vocab_size
        self.hidden_size = hidden_size
        self.num_hidden_layers = num_hidden_layers
        self.num_attention_heads = num_attention_heads
        self.intermediate_size = intermediate_size


class CustomModel(PreTrainedModel):
    """ì»¤ìŠ¤í…€ ëª¨ë¸ êµ¬í˜„"""
    config_class = CustomModelConfig
    
    def __init__(self, config):
        super().__init__(config)
        
        # ëª¨ë¸ ë ˆì´ì–´ ì •ì˜
        self.embed_tokens = nn.Embedding(
            config.vocab_size,
            config.hidden_size
        )
        
        self.layers = nn.ModuleList([
            CustomDecoderLayer(config)
            for _ in range(config.num_hidden_layers)
        ])
        
        self.norm = nn.LayerNorm(config.hidden_size)
        self.lm_head = nn.Linear(
            config.hidden_size,
            config.vocab_size,
            bias=False
        )
    
    def forward(self, input_ids, attention_mask=None, **kwargs):
        # ìˆœì „íŒŒ ë¡œì§
        hidden_states = self.embed_tokens(input_ids)
        
        for layer in self.layers:
            hidden_states = layer(hidden_states, attention_mask)
        
        hidden_states = self.norm(hidden_states)
        logits = self.lm_head(hidden_states)
        
        return {"logits": logits}


# ëª¨ë¸ ë“±ë¡
from transformers import AutoConfig, AutoModelForCausalLM

AutoConfig.register("custom_model", CustomModelConfig)
AutoModelForCausalLM.register(CustomModelConfig, CustomModel)
```

ì‚¬ìš©:

```python
# ì»¤ìŠ¤í…€ ëª¨ë¸ ì„í¬íŠ¸
from src.custom_models import CustomModel, CustomModelConfig

# í•™ìŠµ
python src/train.py --model_name "path/to/custom_model"
```

---

## ë‹¤ë¥¸ í˜•ì‹ ëª¨ë¸ í†µí•©

### GGUF ëª¨ë¸ ì§€ì›

```python
# src/gguf_loader.py (ìƒˆ íŒŒì¼)
"""
GGUF í˜•ì‹ ëª¨ë¸ ë¡œë”
"""

try:
    from llama_cpp import Llama
except ImportError:
    print("llama-cpp-python ì„¤ì¹˜ í•„ìš”: pip install llama-cpp-python")

class GGUFModelWrapper:
    """GGUF ëª¨ë¸ ë˜í¼"""
    
    def __init__(self, model_path: str, n_ctx: int = 2048):
        self.model = Llama(
            model_path=model_path,
            n_ctx=n_ctx,
            n_gpu_layers=-1  # ëª¨ë“  ë ˆì´ì–´ë¥¼ GPUë¡œ
        )
    
    def generate(self, prompt: str, max_tokens: int = 256, **kwargs):
        """í…ìŠ¤íŠ¸ ìƒì„±"""
        output = self.model(
            prompt,
            max_tokens=max_tokens,
            temperature=kwargs.get('temperature', 0.7),
            top_p=kwargs.get('top_p', 0.9),
        )
        
        return output['choices'][0]['text']

# ì‚¬ìš©
from src.gguf_loader import GGUFModelWrapper

model = GGUFModelWrapper("models/llama-2-7b.Q4_K_M.gguf")
response = model.generate("Pythonì´ë€?")
```

### ONNX ëª¨ë¸ ì§€ì›

```python
# src/onnx_loader.py (ìƒˆ íŒŒì¼)
"""
ONNX í˜•ì‹ ëª¨ë¸ ë¡œë”
"""

try:
    import onnxruntime as ort
except ImportError:
    print("onnxruntime ì„¤ì¹˜ í•„ìš”: pip install onnxruntime-gpu")

class ONNXModelWrapper:
    """ONNX ëª¨ë¸ ë˜í¼"""
    
    def __init__(self, model_path: str):
        providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']
        self.session = ort.InferenceSession(model_path, providers=providers)
    
    def generate(self, input_ids, attention_mask=None):
        """ì¶”ë¡  ì‹¤í–‰"""
        inputs = {
            'input_ids': input_ids.numpy(),
        }
        if attention_mask is not None:
            inputs['attention_mask'] = attention_mask.numpy()
        
        outputs = self.session.run(None, inputs)
        return outputs[0]
```

---

## ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ ì§€ì›

### Vision-Language ëª¨ë¸ (LLaVA, BLIP ë“±)

```python
# src/multimodal_utils.py (ìƒˆ íŒŒì¼)
"""
ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ ì§€ì›
"""

from transformers import (
    LlavaForConditionalGeneration,
    AutoProcessor,
    BlipForConditionalGeneration
)
from PIL import Image

class MultiModalModel:
    """ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ ë˜í¼"""
    
    def __init__(self, model_name: str, model_type: str = "llava"):
        self.model_type = model_type
        
        if model_type == "llava":
            self.model = LlavaForConditionalGeneration.from_pretrained(
                model_name,
                device_map="auto"
            )
            self.processor = AutoProcessor.from_pretrained(model_name)
        
        elif model_type == "blip":
            self.model = BlipForConditionalGeneration.from_pretrained(
                model_name,
                device_map="auto"
            )
            self.processor = AutoProcessor.from_pretrained(model_name)
    
    def generate_from_image(
        self,
        image_path: str,
        prompt: str,
        max_new_tokens: int = 256
    ):
        """ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ë¡œë¶€í„° ìƒì„±"""
        image = Image.open(image_path)
        
        inputs = self.processor(
            text=prompt,
            images=image,
            return_tensors="pt"
        ).to(self.model.device)
        
        outputs = self.model.generate(
            **inputs,
            max_new_tokens=max_new_tokens
        )
        
        return self.processor.decode(outputs[0], skip_special_tokens=True)

# ì‚¬ìš©
model = MultiModalModel("llava-hf/llava-1.5-7b-hf", model_type="llava")
response = model.generate_from_image(
    "image.jpg",
    "ì´ ì´ë¯¸ì§€ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”"
)
```

---

## ì‹¤ì „ ì˜ˆì œ

### ì˜ˆì œ 1: Qwen ëª¨ë¸ ì¶”ê°€

```bash
# 1. ê¸°ë³¸ ì‚¬ìš©
python src/train.py \
    --model_name "Qwen/Qwen-7B-Chat" \
    --dataset_path "data/train.json" \
    --use_lora

# 2. trust_remote_code ì„¤ì • í•„ìš” ì‹œ
# configs/train_config.yaml
model:
  name: "Qwen/Qwen-7B-Chat"
  trust_remote_code: true
```

### ì˜ˆì œ 2: Gemma ëª¨ë¸ ì¶”ê°€

```bash
# GemmaëŠ” ë³„ë„ ì¸ì¦ í•„ìš”
huggingface-cli login

# í•™ìŠµ
python src/train.py \
    --model_name "google/gemma-7b" \
    --dataset_path "data/train.json" \
    --use_lora
```

### ì˜ˆì œ 3: ë¡œì»¬ ëª¨ë¸ ì‚¬ìš©

```bash
# 1. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
git clone https://huggingface.co/meta-llama/Llama-2-7b-hf models/llama2

# 2. ë¡œì»¬ ê²½ë¡œë¡œ í•™ìŠµ
python src/train.py \
    --model_name "models/llama2" \
    --dataset_path "data/train.json"
```

### ì˜ˆì œ 4: ì—¬ëŸ¬ ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬

```python
# scripts/benchmark_models.py (ìƒˆ íŒŒì¼)
"""
ì—¬ëŸ¬ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ
"""

import subprocess

MODELS = [
    "meta-llama/Llama-2-7b-hf",
    "mistralai/Mistral-7B-v0.1",
    "google/gemma-7b",
    "Qwen/Qwen-7B"
]

for model in MODELS:
    print(f"\n{'='*60}")
    print(f"ëª¨ë¸: {model}")
    print('='*60)
    
    # í•™ìŠµ
    subprocess.run([
        "python", "src/train.py",
        "--model_name", model,
        "--dataset_path", "data/train.json",
        "--output_dir", f"outputs/{model.split('/')[-1]}",
        "--num_epochs", "1",
        "--use_lora"
    ])
    
    # í‰ê°€
    subprocess.run([
        "python", "scripts/evaluate_model.py",
        "--model_path", f"outputs/{model.split('/')[-1]}",
        "--eval_data", "data/eval.json"
    ])
```

---

## ëª¨ë¸ í˜¸í™˜ì„± í™•ì¸

### ìë™ í˜¸í™˜ì„± ì²´í¬ ìŠ¤í¬ë¦½íŠ¸

```python
# scripts/check_model_compatibility.py (ìƒˆ íŒŒì¼)
"""
ëª¨ë¸ í˜¸í™˜ì„± í™•ì¸
"""

from transformers import AutoTokenizer, AutoModelForCausalLM
import sys

def check_model(model_name: str):
    """ëª¨ë¸ í˜¸í™˜ì„± ì²´í¬"""
    print(f"ëª¨ë¸ í™•ì¸: {model_name}")
    print("-" * 60)
    
    try:
        # í† í¬ë‚˜ì´ì € ì²´í¬
        print("1. í† í¬ë‚˜ì´ì € ë¡œë”©...")
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        print(f"   âœ“ ì„±ê³µ (vocab size: {len(tokenizer)})")
        
        # ëª¨ë¸ ì²´í¬
        print("2. ëª¨ë¸ ë¡œë”©...")
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            device_map="cpu",
            torch_dtype="auto"
        )
        print(f"   âœ“ ì„±ê³µ")
        
        # íŒŒë¼ë¯¸í„° ìˆ˜
        num_params = sum(p.numel() for p in model.parameters())
        print(f"3. íŒŒë¼ë¯¸í„° ìˆ˜: {num_params / 1e9:.2f}B")
        
        # í…ŒìŠ¤íŠ¸ ìƒì„±
        print("4. í…ŒìŠ¤íŠ¸ ìƒì„±...")
        inputs = tokenizer("Hello", return_tensors="pt")
        outputs = model.generate(**inputs, max_new_tokens=10)
        text = tokenizer.decode(outputs[0])
        print(f"   âœ“ ì„±ê³µ: {text[:50]}...")
        
        print("\nâœ… ì´ ëª¨ë¸ì€ í˜¸í™˜ë©ë‹ˆë‹¤!")
        return True
        
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜: {e}")
        return False

if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("ì‚¬ìš©ë²•: python check_model_compatibility.py <model-name>")
        sys.exit(1)
    
    model_name = sys.argv[1]
    check_model(model_name)
```

ì‚¬ìš©:

```bash
python scripts/check_model_compatibility.py "mistralai/Mistral-7B-v0.1"
```

---

## ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬

### ì§€ì› ëª¨ë¸ ëª©ë¡ ê´€ë¦¬

```python
# src/model_registry.py (ìƒˆ íŒŒì¼)
"""
ì§€ì› ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬
"""

SUPPORTED_MODELS = {
    # Meta AI
    "llama-2-7b": {
        "hf_id": "meta-llama/Llama-2-7b-hf",
        "size": "7B",
        "context_length": 4096,
        "license": "Llama 2 License",
        "verified": True
    },
    "llama-2-13b": {
        "hf_id": "meta-llama/Llama-2-13b-hf",
        "size": "13B",
        "context_length": 4096,
        "license": "Llama 2 License",
        "verified": True
    },
    
    # Mistral AI
    "mistral-7b": {
        "hf_id": "mistralai/Mistral-7B-v0.1",
        "size": "7B",
        "context_length": 8192,
        "license": "Apache 2.0",
        "verified": True
    },
    
    # Google
    "gemma-7b": {
        "hf_id": "google/gemma-7b",
        "size": "7B",
        "context_length": 8192,
        "license": "Gemma License",
        "auth_required": True,
        "verified": True
    },
    
    # Alibaba
    "qwen-7b": {
        "hf_id": "Qwen/Qwen-7B",
        "size": "7B",
        "context_length": 8192,
        "license": "Tongyi Qianwen License",
        "trust_remote_code": True,
        "verified": True
    }
}

def list_models():
    """ì§€ì› ëª¨ë¸ ëª©ë¡"""
    print("ì§€ì› ëª¨ë¸ ëª©ë¡:")
    print("=" * 80)
    
    for name, info in SUPPORTED_MODELS.items():
        status = "âœ“" if info.get("verified") else "?"
        auth = " [ì¸ì¦ í•„ìš”]" if info.get("auth_required") else ""
        trust = " [trust_remote_code]" if info.get("trust_remote_code") else ""
        
        print(f"{status} {name:20s} | {info['size']:4s} | {info['license']}{auth}{trust}")

def get_model_info(name: str):
    """ëª¨ë¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°"""
    return SUPPORTED_MODELS.get(name)

if __name__ == "__main__":
    list_models()
```

---

## ìš”ì•½

### ìƒˆ ëª¨ë¸ ì¶”ê°€ ì²´í¬ë¦¬ìŠ¤íŠ¸

1. âœ… **Hugging Faceì—ì„œ ëª¨ë¸ í™•ì¸**
   - https://huggingface.co/models

2. âœ… **í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸**
   ```bash
   python scripts/check_model_compatibility.py "model-name"
   ```

3. âœ… **ê¸°ë³¸ í•™ìŠµ í…ŒìŠ¤íŠ¸**
   ```bash
   python src/train.py \
       --model_name "model-name" \
       --dataset_path "data/train.json" \
       --num_epochs 1
   ```

4. âœ… **ì„¤ì • íŒŒì¼ì— ì¶”ê°€** (ì„ íƒ)
   - configs/models.yaml
   - src/model_registry.py

5. âœ… **ë¬¸ì„œ ì—…ë°ì´íŠ¸**
   - README.mdì— ì§€ì› ëª¨ë¸ ëª©ë¡ ì¶”ê°€

### ëŒ€ë¶€ë¶„ì˜ ê²½ìš°

**ì•„ë¬´ê²ƒë„ ìˆ˜ì •í•  í•„ìš” ì—†ì´** ëª¨ë¸ ì´ë¦„ë§Œ ë³€ê²½í•˜ë©´ ë©ë‹ˆë‹¤:

```bash
python src/train.py --model_name "ìƒˆë¡œìš´-ëª¨ë¸-ì´ë¦„"
```

ì´ê²Œ ì „ë¶€ì…ë‹ˆë‹¤! ğŸš€

