# LLM 파인튜닝 설정 파일

# 모델 설정
model:
  name: "meta-llama/Llama-2-7b-hf"  # Hugging Face 모델 ID
  cache_dir: "./models/cache"  # 모델 캐시 디렉토리
  trust_remote_code: false  # 원격 코드 실행 허용 여부

# 데이터 설정
data:
  train_path: "data/train.json"  # 학습 데이터 경로
  eval_path: "data/eval.json"    # 검증 데이터 경로 (선택사항)
  max_length: 512                 # 최대 시퀀스 길이
  train_split: 0.9                # 학습/검증 분할 비율

# 학습 설정
training:
  output_dir: "outputs/checkpoints"  # 체크포인트 저장 디렉토리
  num_epochs: 3                      # 학습 에포크 수
  batch_size: 4                      # 배치 크기
  gradient_accumulation_steps: 4     # 그래디언트 누적 스텝
  learning_rate: 2.0e-5              # 학습률
  warmup_steps: 100                  # 워밍업 스텝
  weight_decay: 0.01                 # 가중치 감쇠
  max_grad_norm: 1.0                 # 그래디언트 클리핑
  save_steps: 500                    # 체크포인트 저장 주기
  eval_steps: 500                    # 평가 주기
  logging_steps: 10                  # 로깅 주기
  save_total_limit: 3                # 저장할 최대 체크포인트 수

# LoRA 설정 (Parameter-Efficient Fine-Tuning)
lora:
  use_lora: true       # LoRA 사용 여부
  r: 16                # LoRA rank
  lora_alpha: 32       # LoRA alpha
  lora_dropout: 0.05   # LoRA dropout
  target_modules:      # LoRA 적용할 모듈
    - "q_proj"
    - "v_proj"
    - "k_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"
  bias: "none"         # bias 학습 여부

# 양자화 설정
quantization:
  use_quantization: false  # 양자화 사용 여부
  bits: 4                   # 4bit 또는 8bit
  compute_dtype: "float16"  # 계산 데이터 타입

# 옵티마이저 설정
optimizer:
  name: "adamw_torch"  # adamw_torch, adamw_8bit, sgd 등
  lr_scheduler: "cosine"  # linear, cosine, polynomial 등

# 고급 설정
advanced:
  gradient_checkpointing: true  # 그래디언트 체크포인팅 (메모리 절약)
  fp16: false                   # FP16 혼합 정밀도
  bf16: true                    # BF16 혼합 정밀도 (A100/H100 권장)
  dataloader_num_workers: 4     # 데이터 로더 워커 수
  group_by_length: true         # 길이별 그룹화 (효율성 향상)
  ddp_find_unused_parameters: false  # DDP 설정

# 모니터링 설정
monitoring:
  use_wandb: false              # WandB 사용 여부
  wandb_project: "llm-finetuning"  # WandB 프로젝트 이름
  use_tensorboard: true         # TensorBoard 사용 여부
  report_to: ["tensorboard"]    # 리포팅 도구

# 추론 설정
inference:
  max_new_tokens: 256           # 생성할 최대 토큰 수
  temperature: 0.7              # 샘플링 온도
  top_p: 0.9                    # Top-p 샘플링
  top_k: 50                     # Top-k 샘플링
  repetition_penalty: 1.1       # 반복 패널티
  do_sample: true               # 샘플링 사용 여부

