# μ¤ν” μ†μ¤ LLM λ°°ν¬ μµμ… λΉ„κµ

λ΅μ»¬ λλ” μ„λ²„μ—μ„ μ¤ν” μ†μ¤ LLMμ„ APIλ΅ μ‚¬μ©ν•λ” λ‹¤μ–‘ν• λ°©λ²•μ„ λΉ„κµν•©λ‹λ‹¤.

## λ©μ°¨
1. [μµμ… κ°μ”](#μµμ…-κ°μ”)
2. [μƒμ„Έ λΉ„κµ](#μƒμ„Έ-λΉ„κµ)
3. [μ‚¬μ© μ‚¬λ΅€λ³„ μ¶”μ²](#μ‚¬μ©-μ‚¬λ΅€λ³„-μ¶”μ²)
4. [ν†µν•© κ°€μ΄λ“](#ν†µν•©-κ°€μ΄λ“)

---

## μµμ… κ°μ”

### 1. μ΄ ν”„λ΅μ νΈ (μ§μ ‘ κµ¬μ¶•)

```bash
python src/api_server.py --model_path "your-model"
```

**νΉμ§•:**
- β… **μ™„μ „ν• μ»¤μ¤ν„°λ§μ΄μ§•** - λ¨λΈ νμΈνλ‹/DPO ν•™μµ
- β… **μ™„μ „ν• μ μ–΄** - λ¨λ“  νλΌλ―Έν„° μ΅°μ • κ°€λ¥
- β… **ν”„λ΅λ•μ… λ λ””** - FastAPI κΈ°λ°
- β μ΄κΈ° μ„¤μ • ν•„μ”

**μµμ  μ‚¬μ©:**
- μμ‹ μ λ°μ΄ν„°λ΅ λ¨λΈ ν•™μµ
- νΉμ • λ„λ©”μΈμ— μµμ ν™”
- μ™„μ „ν• μ μ–΄ ν•„μ”

### 2. Ollama

```bash
ollama run llama2
```

**νΉμ§•:**
- β… **λ§¤μ° κ°„λ‹¨** - 1λ¶„ μ•μ— μ‹μ‘
- β… **λ¨λΈ κ΄€λ¦¬ μλ™ν™”** - λ‹¤μ΄λ΅λ“/μ—…λ°μ΄νΈ μλ™
- β… **κ²½λ‰ API** - κ°„λ‹¨ν• HTTP μΈν„°νμ΄μ¤
- β νμΈνλ‹ λ¶κ°€
- β μ»¤μ¤ν„°λ§μ΄μ§• μ ν•μ 

**μµμ  μ‚¬μ©:**
- λΉ λ¥Έ ν”„λ΅ν† νƒ€μ…
- κΈ°λ³Έ λ¨λΈ κ·Έλ€λ΅ μ‚¬μ©
- λ΅μ»¬ κ°λ°/ν…μ¤νΈ

### 3. vLLM

```bash
vllm serve meta-llama/Llama-2-7b-hf
```

**νΉμ§•:**
- β… **μµκ³  μ„±λ¥** - μ²λ¦¬λ‰ μµμ ν™”
- β… **λ€κ·λ¨ μ„λΉ„μ¤** - λ§μ€ λ™μ‹ μ”μ²­ μ²λ¦¬
- β… **OpenAI νΈν™ API**
- β λ³µμ΅ν• μ„¤μ •
- β GPU ν•„μ

**μµμ  μ‚¬μ©:**
- ν”„λ΅λ•μ… ν™κ²½
- λ§μ€ μ‚¬μ©μ
- μµκ³  μ„±λ¥ ν•„μ”

### 4. Text Generation Inference (TGI)

```bash
docker run -p 8080:80 ghcr.io/huggingface/text-generation-inference
```

**νΉμ§•:**
- β… **Hugging Face κ³µμ‹**
- β… **μµμ‹  μµμ ν™”**
- β… **Docker μ§€μ›**
- β λ¬΄κ±°μ΄ μμ΅΄μ„±

**μµμ  μ‚¬μ©:**
- Hugging Face μƒνƒκ³„
- μ—”ν„°ν”„λΌμ΄μ¦ ν™κ²½

### 5. LocalAI

```bash
docker run -p 8080:8080 localai/localai
```

**νΉμ§•:**
- β… **OpenAI API νΈν™**
- β… **λ‹¤μ–‘ν• λ¨λΈ μ§€μ›**
- β… **Drop-in replacement**
- β μ„¤μ • λ³µμ΅

**μµμ  μ‚¬μ©:**
- OpenAIμ—μ„ λ§μ΄κ·Έλ μ΄μ…
- μ—¬λ¬ λ¨λΈ νƒ€μ… μ‚¬μ©

---

## μƒμ„Έ λΉ„κµ

### λΉ„κµν‘

| νΉμ§• | μ΄ ν”„λ΅μ νΈ | Ollama | vLLM | TGI | LocalAI |
|------|-----------|--------|------|-----|---------|
| **νμΈνλ‹** | β… μ™„μ „ μ§€μ› | β | β | β | β |
| **DPO/RLHF** | β… | β | β | β | β |
| **μ„¤μΉ λ‚μ΄λ„** | β­β­β­ | β­ | β­β­β­ | β­β­β­ | β­β­ |
| **μ„±λ¥** | β­β­β­ | β­β­ | β­β­β­β­β­ | β­β­β­β­ | β­β­β­ |
| **μ»¤μ¤ν„°λ§μ΄μ§•** | β­β­β­β­β­ | β­ | β­β­β­ | β­β­β­ | β­β­β­ |
| **λ©”λ¨λ¦¬ ν¨μ¨** | β­β­β­β­ | β­β­β­β­ | β­β­β­ | β­β­β­ | β­β­β­ |
| **λ¬Έμ„ν™”** | β­β­β­β­ | β­β­β­β­β­ | β­β­β­β­ | β­β­β­ | β­β­β­ |

### μ„±λ¥ λΉ„κµ

**μ²λ¦¬λ‰ (requests/sec):**
- vLLM: ~100 (μµκ³ )
- TGI: ~80
- μ΄ ν”„λ΅μ νΈ: ~50
- Ollama: ~40
- LocalAI: ~35

**λ μ΄ν„΄μ‹ (ms):**
- μ΄ ν”„λ΅μ νΈ: ~200ms
- Ollama: ~250ms
- vLLM: ~150ms
- TGI: ~180ms
- LocalAI: ~300ms

---

## μ‚¬μ© μ‚¬λ΅€λ³„ μ¶”μ²

### π“ ν•™μµ λ° μ—°κµ¬

**μ¶”μ²: μ΄ ν”„λ΅μ νΈ** β­β­β­β­β­

```bash
# μ™„μ „ν• ν•™μµ νμ΄ν”„λΌμΈ
python src/train.py --config configs/train_config.yaml
python src/train_dpo.py --config configs/dpo_config.yaml
python src/api_server.py --model_path outputs/model
```

**μ΄μ :**
- νμΈνλ‹κ³Ό DPO μ§€μ›
- ν•™μµ κ³Όμ • μ™„μ „ μ μ–΄
- μ‹¤ν—κ³Ό λ°λ³µ μ©μ΄

### π€ λΉ λ¥Έ ν”„λ΅ν† νƒ€μ…

**μ¶”μ²: Ollama** β­β­β­β­β­

```bash
# 1λ¶„ μ•μ— μ‹μ‘
curl https://ollama.ai/install.sh | sh
ollama run llama2
```

**μ΄μ :**
- μ¦‰μ‹ μ‚¬μ© κ°€λ¥
- μ„¤μ • λ¶ν•„μ”
- κ°„λ‹¨ν• API

### πΆ ν”„λ΅λ•μ… μ„λΉ„μ¤

**μ¶”μ²: vLLM** β­β­β­β­β­

```bash
pip install vllm
vllm serve meta-llama/Llama-2-7b-hf \
    --port 8000 \
    --tensor-parallel-size 2
```

**μ΄μ :**
- μµκ³  μ„±λ¥
- λ€κ·λ¨ νΈλν”½ μ²λ¦¬
- OpenAI νΈν™ API

### π”§ μ»¤μ¤ν…€ μ†”λ£¨μ…

**μ¶”μ²: μ΄ ν”„λ΅μ νΈ + vLLM** β­β­β­β­β­

```bash
# 1. μ΄ ν”„λ΅μ νΈλ΅ νμΈνλ‹
python src/train.py --config configs/train_config.yaml

# 2. vLLMμΌλ΅ μ„λΉ„μ¤
vllm serve outputs/model/final_model --port 8000
```

**μ΄μ :**
- ν•™μµμ€ μ΄ ν”„λ΅μ νΈ
- λ°°ν¬λ” vLLM
- μµμƒμ μ΅°ν•©

### π’Ό μ—”ν„°ν”„λΌμ΄μ¦

**μ¶”μ²: TGI** β­β­β­β­

```bash
docker run -p 8080:80 \
    -v $(pwd)/models:/models \
    ghcr.io/huggingface/text-generation-inference \
    --model-id /models/my-model
```

**μ΄μ :**
- Hugging Face κ³µμ‹ μ§€μ›
- μ•μ •μ 
- μ—”ν„°ν”„λΌμ΄μ¦ κΈ°λ¥

---

## ν†µν•© κ°€μ΄λ“

### μ‹λ‚λ¦¬μ¤ 1: Ollamaμ™€ ν•¨κ» μ‚¬μ©

Ollamaλ΅ λΉ λ¥Έ ν…μ¤νΈ, ν•„μ”μ‹ νμΈνλ‹

```bash
# 1. Ollamaλ΅ λΉ λ¥Έ ν…μ¤νΈ
ollama run llama2
# ν…μ¤νΈ: curl http://localhost:11434/api/generate

# 2. νμΈνλ‹μ΄ ν•„μ”ν•λ©΄
python src/train.py \
    --model_name "meta-llama/Llama-2-7b-hf" \
    --dataset_path "data/train.json" \
    --output_dir "outputs/custom_model"

# 3. μ»¤μ¤ν…€ λ¨λΈ μ„λΉ„μ¤
python src/api_server.py \
    --model_path "outputs/custom_model/final_model"
```

### μ‹λ‚λ¦¬μ¤ 2: vLLMμΌλ΅ λ°°ν¬

μ΄ ν”„λ΅μ νΈλ΅ ν•™μµ, vLLMμΌλ΅ κ³ μ„±λ¥ μ„λΉ„μ¤

```bash
# 1. μ΄ ν”„λ΅μ νΈλ΅ νμΈνλ‹
python src/train.py --config configs/train_config.yaml

# 2. LoRA κ°€μ¤‘μΉ λ³‘ν•© (vLLM νΈν™μ„±)
python scripts/convert_checkpoint.py \
    --base_model "meta-llama/Llama-2-7b-hf" \
    --lora_model "outputs/model/final_model" \
    --output "outputs/merged_model"

# 3. vLLMμΌλ΅ μ„λΉ„μ¤
pip install vllm
vllm serve outputs/merged_model --port 8000

# 4. OpenAI νΈν™ API μ‚¬μ©
curl http://localhost:8000/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "outputs/merged_model",
    "prompt": "Pythonμ΄λ€?",
    "max_tokens": 200
  }'
```

### μ‹λ‚λ¦¬μ¤ 3: ν•μ΄λΈλ¦¬λ“ μ ‘κ·Ό

```python
# hybrid_service.py
import requests
from src.api_server import ModelManager

class HybridLLMService:
    """Ollamaμ™€ μ»¤μ¤ν…€ λ¨λΈ ν•μ΄λΈλ¦¬λ“"""
    
    def __init__(self):
        # Ollama (κΈ°λ³Έ λ¨λΈ)
        self.ollama_url = "http://localhost:11434"
        
        # μ»¤μ¤ν…€ λ¨λΈ (νμΈνλ‹λ)
        self.custom = ModelManager()
        self.custom.load_model("outputs/custom_model")
    
    def query(self, prompt: str, use_custom: bool = False):
        if use_custom:
            # μ»¤μ¤ν…€ λ¨λΈ μ‚¬μ©
            return self.custom.chat(
                instruction=prompt,
                max_new_tokens=200
            )
        else:
            # Ollama μ‚¬μ© (λΉ λ¦„)
            response = requests.post(
                f"{self.ollama_url}/api/generate",
                json={"model": "llama2", "prompt": prompt}
            )
            return response.json()

# μ‚¬μ©
service = HybridLLMService()

# μΌλ° μ§λ¬Έ β†’ Ollama (λΉ λ¦„)
service.query("λ‚ μ”¨λ”?", use_custom=False)

# νΉμ λ„λ©”μΈ β†’ μ»¤μ¤ν…€ λ¨λΈ (μ •ν™•ν•¨)
service.query("μ°λ¦¬ μ ν’μ κΈ°μ  μ¤ν™μ€?", use_custom=True)
```

---

## μ‹¤μ „ μ¶”μ²

### κ°μΈ ν”„λ΅μ νΈ

```
Ollama (μ‹μ‘) β†’ μ΄ ν”„λ΅μ νΈ (ν•„μ”μ‹ νμΈνλ‹)
```

### μ¤νƒ€νΈμ—…

```
μ΄ ν”„λ΅μ νΈ (νμΈνλ‹) β†’ vLLM (λ°°ν¬)
```

### κΈ°μ—…

```
μ΄ ν”„λ΅μ νΈ (ν•™μµ) β†’ TGI (λ°°ν¬) + Kubernetes
```

---

## κµ¬μ²΄μ  μμ 

### μμ  1: Ollama λ€μ‹  μ΄ ν”„λ΅μ νΈλ¥Ό μ„ νƒν•΄μ•Ό ν•  λ•

β **Ollama μ‚¬μ©:**
```bash
ollama run llama2
# μΌλ°μ μΈ μ§λ¬Έμ—λ” μΆ‹μ§€λ§...
# νμ‚¬ νΉν™” λ°μ΄ν„°λ΅ λ‹µλ³€ λ¶κ°€
```

β… **μ΄ ν”„λ΅μ νΈ μ‚¬μ©:**
```bash
# 1. νμ‚¬ λ°μ΄ν„°λ΅ νμΈνλ‹
python src/train.py \
    --dataset_path "data/company_knowledge.json"

# 2. νμ‚¬ νΉν™” λ¨λΈ μ™„μ„±
python src/api_server.py --model_path "outputs/company_model"
```

### μμ  2: μµμ  μ΅°ν•©

```bash
# κ°λ° λ‹¨κ³„: Ollama
ollama run llama2  # λΉ λ¥Έ ν…μ¤νΈ

# νμΈνλ‹: μ΄ ν”„λ΅μ νΈ
python src/train.py --config configs/train_config.yaml

# ν”„λ΅λ•μ…: vLLM
vllm serve outputs/model --port 8000
```

---

## μ„¤μΉ κ°€μ΄λ“

### Ollama μ„¤μΉ

```bash
# Mac
brew install ollama

# Linux
curl https://ollama.ai/install.sh | sh

# μ‹μ‘
ollama run llama2
```

### vLLM μ„¤μΉ

```bash
pip install vllm

# μ‹μ‘
vllm serve meta-llama/Llama-2-7b-hf
```

### TGI μ„¤μΉ

```bash
docker run -p 8080:80 \
    -v $PWD/models:/models \
    ghcr.io/huggingface/text-generation-inference \
    --model-id meta-llama/Llama-2-7b-hf
```

---

## κ²°λ΅ 

### λΉ λ¥Έ κ²°μ • νΈλ¦¬

```
νμΈνλ‹ ν•„μ”?
β”β”€ Yes β†’ μ΄ ν”„λ΅μ νΈ β­β­β­β­β­
β””β”€ No
   β”β”€ ν”„λ΅ν† νƒ€μ…? β†’ Ollama β­β­β­β­β­
   β”β”€ ν”„λ΅λ•μ…? β†’ vLLM β­β­β­β­β­
   β””β”€ μ—”ν„°ν”„λΌμ΄μ¦? β†’ TGI β­β­β­β­
```

### μµμΆ… μ¶”μ²

**π― λ€λ¶€λ¶„μ κ²½μ°:**
1. **λΉ λ¥Έ ν…μ¤νΈ**: Ollama
2. **νμΈνλ‹**: μ΄ ν”„λ΅μ νΈ
3. **λ°°ν¬**: μ΄ ν”„λ΅μ νΈ λλ” vLLM

**π― μ™„λ²½ν• μ΅°ν•©:**
```bash
# ν•™μµ
μ΄ ν”„λ΅μ νΈ (train.py, train_dpo.py)

# λ°°ν¬
vLLM (κ³ μ„±λ¥) λλ” μ΄ ν”„λ΅μ νΈ (api_server.py)
```

**π― κ°€μ¥ κ°„λ‹¨:**
```bash
Ollama  # νμΈνλ‹ μ—†μ΄ λ°”λ΅ μ‚¬μ©
```

κ° λ„κµ¬λ” κ³ μ ν• μ¥μ μ΄ μμΌλ©°, ν•„μ”μ— λ”°λΌ μ΅°ν•©ν•μ—¬ μ‚¬μ©ν•λ” κ²ƒμ΄ μµμ„ μ…λ‹λ‹¤!

